{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"database.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_sql_query(\"SELECT * FROM Reviews WHERE Score != 3 LIMIT  5000\",conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### converting score column from 1,2,3,4 to binary  ....1 for 3 and 4 0 for 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participation(x):\n",
    "    if (x>3):\n",
    "        return 1\n",
    "    elif (x<3):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Score2 = list(map(participation,data_set[\"Score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### print(Score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[\"Score\"] = Score2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### now let us perform certain data reforming line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### performing data deplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data_set.sort_values( 'ProductId' , inplace = False , axis = 0 , ascending = True , kind = \"quicksort\" , na_position = \"last\")\n",
    "final = sorted_data.drop_duplicates( subset = {\"UserId\" , \"ProfileName\" , \"Time\" ,\"Text\"} , inplace = False , keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### those data sets for which helpfullness numerator is greater than helpfullness denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final[final[\"HelpfulnessNumerator\"]<=final[\"HelpfulnessDenominator\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why is this $[...] when the same product is available for $[...] here?<br />http://www.amazon.com/VICTOR-FLY-MAGNET-BAIT-REFILL/dp/B00004RBDY<br /><br />The Victor M380 and M502 traps are unreal, of course -- total fly genocide. Pretty stinky, but only right nearby.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[\"Text\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### as we can see that there is a lot of junks in each text which has to be removed before converting it into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3 now it is time to use stopwords and portstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.add(\"br\")    ### as when html tags like < br > </ br > are removed br is not eleminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it', 'couldn', 'below', \"wasn't\", 'while', 'weren', \"hasn't\", 'their', 'off', 'will', \"haven't\", 'up', 'being', 'the', 'didn', \"you've\", 'having', 'at', 'her', 'itself', 'were', \"you'd\", 'through', 'not', 'because', 'yours', 'about', 'own', 'had', 'been', 'again', 'aren', 'am', 'most', 'me', 'during', 'or', 'some', 'what', 'nor', 'herself', 'on', 'of', 'above', 'should', \"mustn't\", 've', 'yourself', 'you', 'only', 'ourselves', 'hadn', 'down', 'y', \"hadn't\", 'isn', 'no', 'further', 're', 'ain', 'hasn', 'out', 'him', 'against', 'd', 'who', 'o', 'was', 's', 'so', 'needn', 'too', 'which', 'theirs', 'our', 'did', 'he', 'is', 'until', 'now', 'does', 'but', 't', 'these', 'shan', 'your', 'doesn', 'once', \"didn't\", 'those', 'we', 'into', \"aren't\", 'before', 'and', 'as', 'in', 'same', \"needn't\", 'this', \"mightn't\", 'a', 'all', 'hers', 'yourselves', 'with', 'each', \"she's\", 'then', 'himself', 'them', 'that', 'be', 'other', 'my', 'are', 'when', 'themselves', 'mightn', 'i', 'over', 'where', 'wasn', 'can', 'br', 'by', 'very', \"weren't\", \"it's\", 'to', 'why', 'under', \"doesn't\", 'whom', 'do', 'his', 'an', 'its', 'll', 'mustn', 'than', 'both', \"shan't\", 'there', 'ours', 'they', 'shouldn', 'between', 'ma', 'more', \"should've\", 'has', 'after', 'wouldn', \"you'll\", 'haven', 'm', 'few', \"that'll\", \"isn't\", 'for', 'how', \"don't\", 'doing', 'she', 'have', 'if', 'won', 'from', \"wouldn't\", 'don', \"shouldn't\", 'any', \"won't\", \"couldn't\", \"you're\", 'such', 'just', 'here', 'myself'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'not'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-d78088d33413>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'not'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nor'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'down'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'not'"
     ]
    }
   ],
   "source": [
    "stop_words.remove('not')\n",
    "stop_words.remove('nor')\n",
    "stop_words.remove('down')\n",
    "stop_words.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem  = nltk.stem.SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasti'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem(\"tasty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using BeautifulSoup for removing html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denocrate(phase):\n",
    "    phase = re.sub(r\"\\'t\",\" not \",phase)\n",
    "    phase = re.sub(r\"\\'ll\", \" will \",phase)\n",
    "    phase = re.sub(r\"\\'re\" , \" are \" , phase)\n",
    "    phase = re.sub(r\"\\'d\" , \" would \" , phase)\n",
    "    phase = re.sub(r\"\\'m\" , \" am \" , phase)\n",
    "    phase = re.sub(r\"\\'ve\" , \" have \" , phase)\n",
    "    phase = re.sub(r\"\\'s\" , \" is \" , phase)\n",
    "    return phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4986/4986 [00:06<00:00, 802.39it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for text in tqdm(final[\"Text\"]):\n",
    "    soup = BeautifulSoup(text , \"lxml\")        ###### remove html tags\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)           #### remove links \n",
    "    text = denocrate(text)\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()     #### removes wods which have deciml numbers\n",
    "    text = re.sub('[^A-Za-z]+', \" \", text).strip()   ####  removes special characters\n",
    "    text = text.lower()\n",
    "    text = ' '.join(word.lower() for word in text.split() if word.lower() not in stop_words)    ## these two lines which removs \n",
    "    ##text = ' '.join(stem.stem(word) for word in text.split() if len(word)>2)                stopwords and stemmerise the words are removed because of tfidf and w2v\n",
    "    text = ' '.join(word for word in text.split() if len(word)>2)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product available victor traps unreal course total fly genocide pretty stinky right nearby'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"Cleaned_text\"] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 11)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning summary of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4986/4986 [00:04<00:00, 1128.35it/s]\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "for text in tqdm(final[\"Summary\"]):\n",
    "    soup = BeautifulSoup(text , \"lxml\")        ###### remove html tags\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)           #### remove links \n",
    "    text = denocrate(text)\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()     #### removes wods which have deciml numbers\n",
    "    text = re.sub('[^A-Za-z]+', \" \", text).strip()   ####  removes special characters\n",
    "   ## text = ' '.join(word.lower() for word in text.split() if word.lower() not in stop_words)\n",
    "    ## text = ' '.join(stem.stem(word) for word in text.split() if len(word)>2)\n",
    "    summary.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"Cleaned_Summary\"] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 12)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[\"Cleaned_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we have a problem that each time we want to use these vectors we have to run ths whole file so we either we could store\n",
    "### the vector converted form of these cleaned texts to excel or database but this is too time consuming so i will store this \n",
    "##3 this cleaned text to new database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn2 = sqlite3.connect(\"final_2.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor_object = conn2.cursor()\n",
    "conn2.text_factory = str\n",
    "final.to_sql('Reviews', conn2 ,schema = None ,if_exists='replace' ,index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now it is time for featurisation(vectorisation of texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 13237)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "count.fit(final[\"Cleaned_text\"])\n",
    "bow_vector = count.transform(final[\"Cleaned_text\"])\n",
    "bow_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trying to store BOW vector into a new excel file\n",
    "bow_vector2 = bow_vector.toarray()\n",
    "bow_df = pd.DataFrame(data=bow_vector2)\n",
    "bow_df.columns = np.arange(13237)\n",
    "bow_df.index = np.arange(4986)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df[\"Score\"] = final[\"Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4  5  6  7  8  9  ...  13228  13229  13230  13231  13232  \\\n",
      "0  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "1  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "2  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "3  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "4  0  0  0  0  0  0  0  0  0  0  ...      0      0      0      0      0   \n",
      "\n",
      "   13233  13234  13235  13236  Score  \n",
      "0      0      0      0      0    1.0  \n",
      "1      0      0      0      0    0.0  \n",
      "2      0      0      0      0    1.0  \n",
      "3      0      0      0      0    0.0  \n",
      "4      0      0      0      0    1.0  \n",
      "\n",
      "[5 rows x 13238 columns]\n"
     ]
    }
   ],
   "source": [
    "print(bow_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  writer = pd.ExcelWriter(\"bow.xlsx\")\n",
    "###  bow_df.to_excel(writer )dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BI-GRAM and TRI-GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4986, 144190)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer(ngram_range= (1,2) )\n",
    "count.fit(final[\"Cleaned_text\"])\n",
    "bigram_vector = count.transform(final[\"Cleaned_text\"])\n",
    "bigram_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(bow_vector))\n",
    "print(type(bigram_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 425)\t1\n",
      "  (0, 570)\t1\n",
      "  (0, 744)\t1\n",
      "  (0, 1524)\t1\n",
      "  (0, 2653)\t1\n",
      "  (0, 4538)\t1\n",
      "  (0, 4582)\t1\n",
      "  (0, 4849)\t1\n",
      "  (0, 5445)\t1\n",
      "  (0, 6074)\t2\n",
      "  (0, 7616)\t1\n",
      "  (0, 7885)\t1\n",
      "  (0, 7951)\t1\n",
      "  (0, 8895)\t1\n",
      "  (0, 8977)\t1\n",
      "  (0, 9785)\t1\n",
      "  (0, 10021)\t1\n",
      "  (0, 11123)\t1\n",
      "  (0, 11766)\t1\n",
      "  (0, 11836)\t1\n",
      "  (0, 12020)\t1\n",
      "  (0, 12090)\t1\n",
      "  (0, 12397)\t1\n",
      "  (0, 12620)\t1\n",
      "  (0, 12900)\t1\n",
      "  (0, 12946)\t1\n",
      "  (0, 5084)\t1\n",
      "  (0, 7124)\t1\n",
      "  (0, 8494)\t1\n",
      "  (0, 9230)\t1\n",
      "  (0, 11100)\t1\n",
      "  (0, 11119)\t1\n",
      "  (0, 19357)\t1\n",
      "  (0, 19646)\t1\n",
      "  (0, 29424)\t1\n",
      "  (0, 29463)\t1\n",
      "  (0, 44987)\t1\n",
      "  (0, 44989)\t1\n",
      "  (0, 45587)\t1\n",
      "  (0, 45949)\t1\n",
      "  (0, 49024)\t1\n",
      "  (0, 49025)\t1\n",
      "  (0, 56459)\t1\n",
      "  (0, 56538)\t1\n",
      "  (0, 61868)\t2\n",
      "  (0, 61963)\t1\n",
      "  (0, 63236)\t1\n",
      "  (0, 79464)\t1\n",
      "  (0, 83146)\t1\n",
      "  (0, 83432)\t1\n",
      "  (0, 86486)\t1\n",
      "  :\t:\n",
      "  (0, 95865)\t1\n",
      "  (0, 96713)\t1\n",
      "  (0, 96849)\t1\n",
      "  (0, 102589)\t1\n",
      "  (0, 102645)\t1\n",
      "  (0, 104066)\t1\n",
      "  (0, 104161)\t1\n",
      "  (0, 114080)\t1\n",
      "  (0, 114082)\t1\n",
      "  (0, 120579)\t1\n",
      "  (0, 122782)\t1\n",
      "  (0, 125907)\t1\n",
      "  (0, 126658)\t1\n",
      "  (0, 130004)\t1\n",
      "  (0, 130013)\t1\n",
      "  (0, 130375)\t1\n",
      "  (0, 130376)\t1\n",
      "  (0, 132478)\t1\n",
      "  (0, 132480)\t1\n",
      "  (0, 135010)\t1\n",
      "  (0, 135011)\t1\n",
      "  (0, 138726)\t1\n",
      "  (0, 138956)\t1\n",
      "  (0, 139818)\t1\n",
      "  (0, 139858)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow_vector[0])\n",
    "print(bigram_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some feature names of the tfidf are ['aa', 'ability', 'ability to', 'able', 'able to', 'about', 'about all', 'about an', 'about and', 'about any']\n",
      "the shape of tfidf vector is (4986, 41401)\n"
     ]
    }
   ],
   "source": [
    "tfidf  = TfidfVectorizer(ngram_range = (1,2) , min_df = 2)\n",
    "tfidf_model = tfidf.fit(final[\"Cleaned_text\"])\n",
    "print(\"some feature names of the tfidf are\" ,tfidf_model.get_feature_names()[0:10])\n",
    "\n",
    "tfidf_vector = tfidf_model.transform(final[\"Cleaned_text\"])\n",
    "print(\"the shape of tfidf vector is\", tfidf_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = list(tfidf.idf_)               ### this is how list and idf_ works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.415977516878012"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 40082)\t0.23831409902091796\n",
      "  (0, 40069)\t0.14045952107997586\n",
      "  (0, 39756)\t0.17479555829616217\n",
      "  (0, 39676)\t0.08597027288899661\n",
      "  (0, 38410)\t0.25970350402276177\n",
      "  (0, 37742)\t0.25970350402276177\n",
      "  (0, 37248)\t0.25970350402276177\n",
      "  (0, 37169)\t0.199655852610952\n",
      "  (0, 36007)\t0.2196097248962045\n",
      "  (0, 35684)\t0.045489675095792007\n",
      "  (0, 34555)\t0.12619969350218824\n",
      "  (0, 33577)\t0.036932048841295875\n",
      "  (0, 31722)\t0.233557257451612\n",
      "  (0, 29140)\t0.22580209918210178\n",
      "  (0, 29120)\t0.12119141898384185\n",
      "  (0, 28756)\t0.11950474845365835\n",
      "  (0, 27284)\t0.1479996374442367\n",
      "  (0, 27230)\t0.08238493343280165\n",
      "  (0, 26965)\t0.13169020145135227\n",
      "  (0, 24556)\t0.09032373108648338\n",
      "  (0, 23592)\t0.16078839598592196\n",
      "  (0, 23475)\t0.048221737935928925\n",
      "  (0, 22373)\t0.21003884361605635\n",
      "  (0, 17856)\t0.1955352890172303\n",
      "  (0, 17346)\t0.21003884361605635\n",
      "  (0, 17304)\t0.08550333686272236\n",
      "  (0, 15858)\t0.12526249036402934\n",
      "  (0, 12686)\t0.0514356034401219\n",
      "  (0, 12500)\t0.24394024845671633\n",
      "  (0, 8504)\t0.15838262844934103\n",
      "  (0, 5865)\t0.21692469401907413\n",
      "  (0, 5744)\t0.05953535827711068\n",
      "  (0, 3548)\t0.21445470504719316\n",
      "  (0, 3542)\t0.14292951005185686\n",
      "  (0, 2679)\t0.061259559034438124\n",
      "  (0, 1501)\t0.03730343022123345\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text =final[\"Cleaned_text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sentence = []\n",
    "for sentence in final[\"Cleaned_text\"]:\n",
    "    list_of_sentence.append(sentence.split())\n",
    "w2v = Word2Vec(list_of_sentence , size= 50 , min_count = 5)\n",
    "w2v_words = list(w2v.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3989\n"
     ]
    }
   ],
   "source": [
    "print(len(w2v_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.9517245292663574),\n",
       " ('crunchy', 0.9134570360183716),\n",
       " ('flavorful', 0.8984339237213135),\n",
       " ('light', 0.8863414525985718),\n",
       " ('fresh', 0.8853623867034912),\n",
       " ('fluffy', 0.8820385336875916),\n",
       " ('pretty', 0.8816474676132202),\n",
       " ('very', 0.8756937384605408),\n",
       " ('soft', 0.874908447265625),\n",
       " ('gooey', 0.8735905885696411)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"tasty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average w2v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vector = []\n",
    "for sentance in list_of_sentence:\n",
    "    sen_vector = np.zeros(50)\n",
    "    count = 0\n",
    "    for word in sentance:\n",
    "        if word in w2v_words:\n",
    "            count = count + 1\n",
    "            sen_vector = sen_vector + w2v.wv[word]\n",
    "        if count != 0:\n",
    "            sen_vector = sen_vector/count\n",
    "    total_vector.append(sen_vector)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_vector[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.08156013e-04 -6.78834132e-03  7.09623524e-04 -7.53929354e-03\n",
      " -3.43534657e-05 -9.72903669e-04  1.27452140e-02 -3.37219166e-03\n",
      " -4.28336582e-03 -1.14290876e-02 -4.60711825e-03  7.88136305e-03\n",
      "  4.40770270e-03  3.21531116e-03 -3.86610504e-03 -3.39473615e-04\n",
      "  1.34545079e-03 -3.96073239e-03 -5.01245441e-03  5.11601480e-03\n",
      " -6.90710922e-03 -3.45971101e-03 -8.21304147e-04 -2.50068791e-04\n",
      "  2.49394725e-03 -7.43398182e-03  9.33497015e-03  6.62442383e-03\n",
      " -1.17566965e-02 -9.38266267e-04  1.27280022e-03 -7.22660486e-03\n",
      " -1.02415070e-03 -1.51319437e-03  6.55179777e-04  5.02812712e-03\n",
      " -8.29157207e-03 -2.03944049e-03 -1.10208936e-02  8.77874020e-03\n",
      " -3.26112182e-03 -5.50128051e-03 -2.01986268e-03 -7.20346151e-04\n",
      "  1.09494594e-02  2.34320317e-03  2.03369980e-03 -8.23537576e-04\n",
      " -1.08338685e-02  5.77258272e-03]\n"
     ]
    }
   ],
   "source": [
    "print(total_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tfidf_weighted_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TfidfVectorizer()\n",
    "model.fit(final[\"Cleaned_text\"])\n",
    "#### now we are zipping words with their idf_ value pay attention it is not tfidf value\n",
    "dictionary = dict(zip(model.get_feature_names(),list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/4986 [00:00<?, ?it/s]<ipython-input-73-ddfe75708ec8>:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sentance_vector /= tfidf_sum\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4986/4986 [04:35<00:00, 18.09it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf_weighted_w2v = []\n",
    "tfidf_feat = model.get_feature_names()\n",
    "for sent in tqdm(list_of_sentence):\n",
    "    sentance_vector = np.zeros(50)\n",
    "    tfidf_sum = 0\n",
    "    for word in sent:\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            tfidf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sentance_vector += (w2v.wv[word]*tfidf)\n",
    "            tfidf_sum += tfidf\n",
    "        if tfidf != 0:\n",
    "            sentance_vector /= tfidf_sum\n",
    "    tfidf_weighted_w2v.append(sentance_vector)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_weighted_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
